{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "390dfab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB version: 1.7.6\n",
      "negative breaks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from numpy import nan\n",
    "import xgboost as xgb\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "#print(cwd)\n",
    "\n",
    "# DRAC directory\n",
    "#os.chdir(\"/home/georod/projects/def-mfortin/georod/scripts/github/forc_trends/models/xgboost\")\n",
    "# Win directory\n",
    "os.chdir(r'C:\\Users\\Peter R\\github\\forc_stpa\\models\\xgboost\\exploratory')\n",
    "\n",
    "\n",
    "print(\"XGB version:\", xgb.__version__)\n",
    "print(\"negative breaks\")\n",
    "\n",
    "\n",
    "# Windows\n",
    "df1 = pd.read_csv(r'.\\data\\df2_trends_p1_vars_greening_v1.csv', skipinitialspace=True)\n",
    "# DRAC\n",
    "#df1 = pd.read_csv(r'./data/forest_evi_breaks_positive_v2.csv', skipinitialspace=True)\n",
    "#df1.head()\n",
    "\n",
    "\n",
    "df11 = pd.get_dummies(df1, columns=['for_pro'], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c866a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF columns + protected\n",
    "cols2 = ['delta_dd5_wt_lag3', 'delta_cmi_sm_lag2', 'delta_cmi_sm_lag1', 'delta_for_age', 'delta_for_con','for_pro_0']\n",
    "df0 = df11.loc[(df11['for_age_2003'] >= 0) & (df11['elev'] > 0)]\n",
    "dfall = [df0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e84d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in range(len(dfall)):\n",
    "    #list_of_vars = [[cols1], [cols2], [cols3]]\n",
    "    list_of_vars = [[cols2]]\n",
    "    for index, list in enumerate(list_of_vars):\n",
    "        for x in list:\n",
    "            #print(x)\n",
    "            X1 = dfall[z][x]\n",
    "            #print(X1.describe())\n",
    "            y1 = dfall[z].iloc[:,6].abs()\n",
    "            seed = 7 # random seed to help with replication\n",
    "            testsize1 = 0.33 # percent of records to test after training\n",
    "            x1_train, x1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=testsize1, random_state=seed) # Split data set. Note the 'stratify' option\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc791c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune parameters using GridSearchCV (for exhaustive searches) or RandomizedSearchCV (faster)\n",
    "# max_depth is tree complexity in Elith et al. 2008\n",
    "# n_estimators=100 is the number of trees. Elith et al. 2008 say this should be 1000 at least\n",
    "# Elith et al. 2008 suggests low learning rate\n",
    "\n",
    "params_xgboost = {\n",
    " #\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n",
    " \"learning_rate\"    : [ 0.01, 0.05, 0.10, 0.15, 0.20, 0.25],\n",
    " #\"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    " \"max_depth\"        : [ 3, 4, 5, 6, 8],\n",
    " #\"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    " #\"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    " \"gamma\"            : [ 0.0, 0.05, 0.1, 0.2, 0.3, 0.4],\n",
    " #\"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7],\n",
    " #'n_estimators'     : [5, 10, 15, 20, 25, 30, 35],\n",
    "'n_estimators'     : [300],\n",
    " 'objective': ['reg:squarederror'],\n",
    "#'early_stopping_rounds': [10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "919c5244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=6, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n"
     ]
    }
   ],
   "source": [
    "model_m2b = XGBRegressor()\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator = model_m2b, \n",
    "                      param_distributions = params_xgboost, \n",
    "                      n_iter = 100, \n",
    "                      cv = 5, \n",
    "                      verbose=10, \n",
    "                      random_state=42,\n",
    "                      scoring = 'neg_mean_squared_error', \n",
    "                      n_jobs = -1)\n",
    "\n",
    "#params glare proba\n",
    "random_search.fit(x1_train, y1_train)\n",
    "\n",
    "print(random_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d80cee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(random_search.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4db593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters from best model used in Chapter 2\n",
    "#XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
    "                 colsample_bylevel=None, colsample_bynode=None,\n",
    "                 colsample_bytree=None, early_stopping_rounds=50,\n",
    "                 enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "                 gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "                 interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
    "                 max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "                 max_delta_step=None, max_depth=8, max_leaves=None,\n",
    "                 min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "                 n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n",
    "                 predictor=None, random_state=42, reg_lambda=10, reg_alpha=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
